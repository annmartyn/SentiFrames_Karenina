{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9f424b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yutam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json, nltk\n",
    "nltk.download('punkt')\n",
    "from langdetect import detect\n",
    "import stanza \n",
    "from nltk.parse import DependencyGraph\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7eb8f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "927d45f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "\n",
    "names_extractor = NamesExtractor(morph_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8dbdca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('annakarenina.txt', 'r', encoding='utf-8') as ak:\n",
    "    text = ak.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0f9c4a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytext = '''Анна любила Вронского, но он не давал ей надежды на счастливое будущее.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2193c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Doc(mytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "938f3ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)\n",
    "for token in doc.tokens:\n",
    "    token.lemmatize(morph_vocab)\n",
    "doc.parse_syntax(syntax_parser)\n",
    "doc.tag_ner(ner_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ae65b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = ''\n",
    "last = 0\n",
    "for span in doc.ner.spans:\n",
    "    if span.type == 'PER':\n",
    "        name = ''\n",
    "        start, stop = span.start, span.stop\n",
    "        name =  mytext[start:stop]\n",
    "        ana = morph.parse(name)\n",
    "        ana[0].tag.gender\n",
    "        name = name.replace(' ', '_')\n",
    "        new_text += mytext[last:start] + name + '_PER' + '_' + ana[0].tag.gender\n",
    "        last = span.stop\n",
    "new_text += mytext[last:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "94d653fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Анна_PER_femn любила Вронского_PER_masc, но он не давал ей надежды на счастливое будущее.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f729b053",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_new = []\n",
    "from string import punctuation\n",
    "my_tokens = []\n",
    "for t in new_text.split():\n",
    "    if t[-1] in punctuation:\n",
    "        my_tokens.append(t[:-1])\n",
    "        my_tokens.append(t[-1])\n",
    "    else:\n",
    "        my_tokens.append(t)\n",
    "for numb, token in enumerate(my_tokens):\n",
    "    if morph.parse(token)[0].tag.POS == 'NPRO':\n",
    "      #  print(token, morph.parse(token)[0].tag.gender)\n",
    "        my_tokens[numb] = 'PRON_' + morph.parse(token)[0].tag.gender + '_' + morph.parse(token)[0].tag.case\n",
    "for r in enumerate(list(reversed(my_tokens))):\n",
    "   # print(r)\n",
    "    if 'PRON_masc' in r[1]:\n",
    "        for potential_name in list(reversed(my_tokens))[r[0]:]:\n",
    "            if potential_name.endswith('masc'):\n",
    "                new_new.append(potential_name + r[1][-5:])\n",
    "                break\n",
    "    elif 'PRON_femn' in r[1]:\n",
    "        for potential_name in list(reversed(my_tokens))[r[0]:]:\n",
    "            if potential_name.endswith('femn'):\n",
    "                new_new.append(potential_name + r[1][-5:])\n",
    "                break\n",
    "    else:\n",
    "        new_new.append(r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "01ec7ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " 'будущее',\n",
       " 'счастливое',\n",
       " 'на',\n",
       " 'надежды',\n",
       " 'Анна_PER_femn_datv',\n",
       " 'давал',\n",
       " 'не',\n",
       " 'Вронского_PER_masc_nomn',\n",
       " 'но',\n",
       " ',',\n",
       " 'Вронского_PER_masc',\n",
       " 'любила',\n",
       " 'Анна_PER_femn']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f4b8d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_text = []\n",
    "for word in new_new:\n",
    "    if word.endswith('femn') or word.endswith('masc'):\n",
    "        w = word.split('_')\n",
    "        last_text.append(' '.join(w[:-2]))\n",
    "    elif 'Кити' in word:\n",
    "        last_text.append('Кити')\n",
    "    elif 'Долли' in word:\n",
    "        last_text.append('Долли')\n",
    "    elif len(word) > 6 and word[-5] == '_':\n",
    "        w = word.split('_')\n",
    "        name = ' '.join(w[:-3])\n",
    "        case = w[-1]\n",
    "        n = morph.parse(name)[0]\n",
    "        f = n.inflect({case})\n",
    "        last_text.append(f.word.title())\n",
    "    else:\n",
    "        last_text.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e23f04ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " 'будущее',\n",
       " 'счастливое',\n",
       " 'на',\n",
       " 'надежды',\n",
       " 'Анне',\n",
       " 'давал',\n",
       " 'не',\n",
       " 'Вронский',\n",
       " 'но',\n",
       " ',',\n",
       " 'Вронского',\n",
       " 'любила',\n",
       " 'Анна']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "afeb67c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ''\n",
    "for a in list(reversed(last_text)):\n",
    "    if a in [',', '.', ':', ';', '!', '?']:\n",
    "        words += a\n",
    "    else:\n",
    "        words += ' ' + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c41fb1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Анна любила Вронского, но Вронский не давал Анне надежды на счастливое будущее.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "12e60386",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [56]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc1\u001b[38;5;241m.\u001b[39mtokens:\n\u001b[0;32m      5\u001b[0m     token\u001b[38;5;241m.\u001b[39mlemmatize(morph_vocab)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mdoc1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_syntax\u001b[49m\u001b[43m(\u001b[49m\u001b[43msyntax_parser\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m doc1\u001b[38;5;241m.\u001b[39mtag_ner(ner_tagger)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\natasha\\doc.py:139\u001b[0m, in \u001b[0;36mDoc.parse_syntax\u001b[1;34m(self, parser)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_syntax\u001b[39m(\u001b[38;5;28mself\u001b[39m, parser):\n\u001b[1;32m--> 139\u001b[0m     \u001b[43mparse_syntax_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\natasha\\doc.py:239\u001b[0m, in \u001b[0;36mparse_syntax_doc\u001b[1;34m(doc, parser)\u001b[0m\n\u001b[0;32m    237\u001b[0m chunk \u001b[38;5;241m=\u001b[39m [sent_words(_) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents]\n\u001b[0;32m    238\u001b[0m markups \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mmap(chunk)\n\u001b[1;32m--> 239\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent_id, (sent, markup) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(doc\u001b[38;5;241m.\u001b[39msents, markups), \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    240\u001b[0m     inject_syntax(sent\u001b[38;5;241m.\u001b[39mtokens, markup\u001b[38;5;241m.\u001b[39mtokens)\n\u001b[0;32m    241\u001b[0m     offset_syntax(sent_id, sent\u001b[38;5;241m.\u001b[39mtokens)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\natasha\\syntax.py:79\u001b[0m, in \u001b[0;36mSyntaxParser.map\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, items):\n\u001b[0;32m     78\u001b[0m     markups \u001b[38;5;241m=\u001b[39m SlovnetSyntax\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mself\u001b[39m, items)\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m markup \u001b[38;5;129;01min\u001b[39;00m markups:\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m adapt_markup(markup)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\slovnet\\api.py:35\u001b[0m, in \u001b[0;36mAPI.map\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, items):\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chop(items, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size):\n\u001b[1;32m---> 35\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(chunk)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\slovnet\\exec\\infer.py:109\u001b[0m, in \u001b[0;36mSyntaxInfer.__call__\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m    106\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess(inputs)\n\u001b[0;32m    107\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(preds)\n\u001b[1;32m--> 109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item, pred \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(items, preds):\n\u001b[0;32m    110\u001b[0m     ids, head_ids, rels \u001b[38;5;241m=\u001b[39m pred\n\u001b[0;32m    111\u001b[0m     tuples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(ids, item, head_ids, rels)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\slovnet\\exec\\infer.py:82\u001b[0m, in \u001b[0;36mSyntaxDecoder.__call__\u001b[1;34m(self, preds)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, preds):\n\u001b[1;32m---> 82\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m preds:\n\u001b[0;32m     83\u001b[0m         head_ids, rel_ids \u001b[38;5;241m=\u001b[39m pred\n\u001b[0;32m     84\u001b[0m         ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(_ \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(head_ids))]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\slovnet\\exec\\infer.py:93\u001b[0m, in \u001b[0;36mSyntaxInfer.process\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs:\n\u001b[1;32m---> 93\u001b[0m         pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m         mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mpad_mask\n\u001b[0;32m     96\u001b[0m         head_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mhead\u001b[38;5;241m.\u001b[39mdecode(pred\u001b[38;5;241m.\u001b[39mhead_id, mask)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\slovnet\\exec\\model.py:477\u001b[0m, in \u001b[0;36mSyntax.__call__\u001b[1;34m(self, word_id, shape_id, pad_mask)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, word_id, shape_id, pad_mask):\n\u001b[0;32m    476\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb(word_id, shape_id)\n\u001b[1;32m--> 477\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m     head_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(x)\n\u001b[0;32m    480\u001b[0m     target_head_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead\u001b[38;5;241m.\u001b[39mdecode(head_id, \u001b[38;5;241m~\u001b[39mpad_mask)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\slovnet\\exec\\model.py:282\u001b[0m, in \u001b[0;36mCNNEncoder.__call__\u001b[1;34m(self, input, mask)\u001b[0m\n\u001b[0;32m    279\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(mask, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28minput\u001b[39m[mask\u001b[38;5;241m.\u001b[39mrepeat(size, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\slovnet\\exec\\model.py:266\u001b[0m, in \u001b[0;36mCNNEncoderLayer.__call__\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m--> 266\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\slovnet\\exec\\model.py:108\u001b[0m, in \u001b[0;36mConv1d.__call__\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    106\u001b[0m windows \u001b[38;5;241m=\u001b[39m windows\u001b[38;5;241m.\u001b[39mreshape(batch_size \u001b[38;5;241m*\u001b[39m windows_count, in_dim \u001b[38;5;241m*\u001b[39m kernel_size)\n\u001b[0;32m    107\u001b[0m weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mreshape(filters_count, in_dim \u001b[38;5;241m*\u001b[39m kernel_size)\n\u001b[1;32m--> 108\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39marray\n\u001b[0;32m    109\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mreshape(batch_size, windows_count, filters_count)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# as in torch\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "doc1 = Doc(text)\n",
    "doc1.segment(segmenter)\n",
    "doc1.tag_morph(morph_tagger)\n",
    "for token in doc1.tokens:\n",
    "    token.lemmatize(morph_vocab)\n",
    "doc1.parse_syntax(syntax_parser)\n",
    "doc1.tag_ner(ner_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b06868a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remake_paragraph(text):\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    for token in doc.tokens:\n",
    "        token.lemmatize(morph_vocab)\n",
    "    doc.parse_syntax(syntax_parser)\n",
    "    doc.tag_ner(ner_tagger)\n",
    "    new_text = ''\n",
    "    last = 0\n",
    "    for span in doc.ner.spans:\n",
    "        if span.type == 'PER':\n",
    "            name = ''\n",
    "            start, stop = span.start, span.stop\n",
    "            name =  text[start:stop]\n",
    "            ana = morph.parse(name)\n",
    "            if ana[0].tag.gender != None:\n",
    "                name = name.replace(' ', '_')\n",
    "                new_text += text[last:start] + name + '_PER' + '_' + ana[0].tag.gender\n",
    "                last = span.stop\n",
    "    new_text += text[last:]\n",
    "    new_new = []\n",
    "    from string import punctuation\n",
    "    my_tokens = []\n",
    "    for t in new_text.split():\n",
    "        if t[-1] in punctuation:\n",
    "            my_tokens.append(t[:-1])\n",
    "            my_tokens.append(t[-1])\n",
    "        else:\n",
    "            my_tokens.append(t)\n",
    "    for numb, token in enumerate(my_tokens):\n",
    "        if morph.parse(token)[0].tag.POS == 'NPRO':\n",
    "            if morph.parse(token)[0].tag.gender in ['masc', 'femn'] and morph.parse(token)[0].tag.case != None:\n",
    "                my_tokens[numb] = 'PRON_' + morph.parse(token)[0].tag.gender + '_' + morph.parse(token)[0].tag.case\n",
    "    for r in enumerate(list(reversed(my_tokens))):\n",
    "        if 'PRON_masc' in r[1]:\n",
    "            for potential_name in list(reversed(my_tokens))[r[0]:]:\n",
    "                if potential_name.endswith('masc'):\n",
    "                    new_new.append(potential_name + r[1][-5:])\n",
    "                    break\n",
    "        elif 'PRON_femn' in r[1]:\n",
    "            for potential_name in list(reversed(my_tokens))[r[0]:]:\n",
    "                if potential_name.endswith('femn'):\n",
    "                    new_new.append(potential_name + r[1][-5:])\n",
    "                    break\n",
    "        else:\n",
    "            new_new.append(r[1])\n",
    "    last_text = []\n",
    "   # print(new_new)\n",
    "    for word in new_new:\n",
    "        if word.endswith('femn') or word.endswith('masc'):\n",
    "            w = word.split('_')\n",
    "            last_text.append(' '.join(w[:-2]))\n",
    "        elif 'Кити' in word:\n",
    "            last_text.append('Кити')\n",
    "        elif 'Долли' in word:\n",
    "            last_text.append('Долли')\n",
    "        elif len(word) > 6 and word[-5] == '_':\n",
    "            w = word.split('_')\n",
    "            name = ' '.join(w[:-3])\n",
    "            case = w[-1]\n",
    "            n = morph.parse(name)[0]\n",
    "            f = n.inflect({case})\n",
    "            if f != None:\n",
    "                last_text.append(f.word.title())\n",
    "            else:\n",
    "                last_text.append(w[0])\n",
    "        else:\n",
    "            last_text.append(word)\n",
    "    words = ''\n",
    "    for a in list(reversed(last_text)):\n",
    "        if a in [',', '.', ':', ';', '!', '?']:\n",
    "            words += a\n",
    "        else:\n",
    "            words += ' ' + a\n",
    "    return words.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "560b4d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "newbook = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cd070193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7807/7807 [06:21<00:00, 20.44it/s]\n"
     ]
    }
   ],
   "source": [
    "for t in tqdm(paragraphs):\n",
    "    if len(t) > 3:\n",
    "       # print(remake_paragraph(t))\n",
    "        newbook += remake_paragraph(t) + '\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7f9eeab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new_ak.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(newbook)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
